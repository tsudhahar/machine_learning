{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working a Text-Based Data Science Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Reading in and exploring the data\n",
    "2. Feature engineering\n",
    "3. Model evaluation using train_test_split and cross_val_score\n",
    "4. Making predictions for new data\n",
    "5. Searching for optimal tuning parameters using GridSearchCV\n",
    "6. Extracting features from text using CountVectorizer\n",
    "7. Proper cross-validation using Pipeline\n",
    "8. Combining GridSearchCV with Pipeline\n",
    "9. Efficiently searching for tuning parameters using RandomizedSearchCV\n",
    "10. Ensembling\n",
    "11. Locating groups of similar cuisines\n",
    "12. Model stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use print only as a function\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading in and exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_json('data/train.json')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the number of null values in each column\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select row 0, column 'ingredients'\n",
    "train.loc[0, 'ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# contents are stored as a list of strings, not as a string\n",
    "type(train.loc[0, 'ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the class distribution\n",
    "train.cuisine.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the number of ingredients in each recipe\n",
    "train['num_ingredients'] = train.ingredients.apply(len)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each cuisine, calculate the mean number of ingredients\n",
    "train.groupby('cuisine').num_ingredients.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each cuisine, \"describe\" the number of ingredients (and unstack into a DataFrame)\n",
    "#train.groupby('cuisine').num_ingredients.describe()\n",
    "train.groupby('cuisine').num_ingredients.describe().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allow plots to appear in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# box plot of number ingredients for each cuisine\n",
    "train.boxplot('num_ingredients', by='cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the mean ingredient length for each recipe\n",
    "train['ingredient_length'] = train.ingredients.apply(lambda x: np.mean([len(item) for item in x]))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# box plot of ingredient length for each cuisine\n",
    "train.boxplot('ingredient_length', by='cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a DataFrame and adds new features\n",
    "def make_features(df):\n",
    "    df['num_ingredients'] = df.ingredients.apply(len)\n",
    "    df['ingredient_length'] = df.ingredients.apply(lambda x: np.mean([len(item) for item in x]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that the function works\n",
    "train = make_features(pd.read_json('data/train.json'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model evaluation using train_test_split and cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['num_ingredients', 'ingredient_length']\n",
    "X = train[feature_cols]\n",
    "y = train.cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use KNN with K=100\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make class predictions for the testing set\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_class = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the classification accuracy of KNN's predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use 5-fold cross-validation instead of train/test split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(knn, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the null accuracy\n",
    "y.value_counts().head(1) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Making predictions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in test.json and add the additional features\n",
    "new = make_features(pd.read_json('data/test.json'))\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train KNN on all of the data\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a DataFrame of the relevant columns from the new data\n",
    "X_new = new[feature_cols]\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make class predictions for the new data\n",
    "new_pred_class_knn = knn.predict(X_new)\n",
    "new_pred_class_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for the new data (for use with ensembling)\n",
    "new_pred_prob_knn = knn.predict_proba(X_new)\n",
    "new_pred_prob_knn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a DataFrame that only contains the IDs and predicted classes for the new data\n",
    "pd.DataFrame({'id':new.id, 'cuisine':new_pred_class_knn}).set_index('id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a submission file from that DataFrame (score: 0.21742)\n",
    "pd.DataFrame({'id':new.id, 'cuisine':new_pred_class_knn}).set_index('id').to_csv('sub1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Searching for optimal tuning parameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recalculate the cross-validated accuracy of KNN with K=100\n",
    "knn = KNeighborsClassifier(n_neighbors=100)\n",
    "cross_val_score(knn, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a \"parameter grid\" in which the key is the parameter and the value is a list of options to try\n",
    "param_grid = {}\n",
    "param_grid['n_neighbors'] = [100, 200, 300]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the grid\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run the grid search\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the scores for each parameter option\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try K=100 to 1000 (by 100)\n",
    "param_grid = {}\n",
    "param_grid['n_neighbors'] = range(100, 1001, 100)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time the grid search using an IPython \"magic function\"\n",
    "%time grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the scores for each parameter option\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract only the mean scores\n",
    "grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "grid_mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# line plot of K value (x-axis) versus accuracy (y-axis)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(100, 1001, 100), grid_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the single best score and parameters that produced that score\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Extracting features from text using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reminder: contents are stored as a list of strings, not as a string\n",
    "train.loc[10, 'ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update make_features to create a new column 'ingredients_str'\n",
    "def make_features(df):\n",
    "    df['num_ingredients'] = df.ingredients.apply(len)\n",
    "    df['ingredient_length'] = df.ingredients.apply(lambda x: np.mean([len(item) for item in x]))\n",
    "    df['ingredients_str'] = df.ingredients.astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run make_features and check that it worked\n",
    "train = make_features(train)\n",
    "train.loc[0, 'ingredients_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = train.ingredients_str\n",
    "y = train.cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a document-term matrix from all of the training data\n",
    "X_dtm = vect.fit_transform(X)\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the features that were created\n",
    "print(vect.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace the regex pattern that is used for tokenization\n",
    "vect = CountVectorizer(token_pattern=r\"'([a-z ]+)'\")\n",
    "X_dtm = vect.fit_transform(X)\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the features that were created\n",
    "print(vect.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Proper cross-validation using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate Multinomial Naive Bayes (with the default parameters)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# slightly improper cross-validation\n",
    "cross_val_score(nb, X_dtm, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a \"pipeline\" of CountVectorizer and MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the pipeline steps\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# proper cross-validation with pipeline\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Combining GridSearchCV with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a grid of parameters to search (and specify the pipeline step along with the parameter)\n",
    "param_grid = {}\n",
    "param_grid['countvectorizer__token_pattern'] = [r\"\\b\\w\\w+\\b\", r\"'([a-z ]+)'\"]\n",
    "param_grid['multinomialnb__alpha'] = [0, 0.5, 1]\n",
    "param_grid\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pass the pipeline (instead of just the model) to GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
    "%time grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# examine the scores for each combination of parameters\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Efficiently searching for tuning parameters using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for any continuous parameters, specify a distribution instead of a list of options\n",
    "import scipy as sp\n",
    "param_grid = {}\n",
    "param_grid['countvectorizer__token_pattern'] = [r\"\\b\\w\\w+\\b\", r\"'([a-z ]+)'\"]\n",
    "param_grid['countvectorizer__min_df'] = [1, 2, 3, 4]\n",
    "param_grid['multinomialnb__alpha'] = sp.stats.uniform(scale=1)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set a random seed for sp.stats.uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run 10 random searches\n",
    "rand = RandomizedSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_iter=10, random_state=1)\n",
    "%time rand.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rand.best_score_)\n",
    "print(rand.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run make_features on the new data\n",
    "new = make_features(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X_new as the ingredient text\n",
    "X_new = new.ingredients_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RandomizedSearchCV (and GridSearchCV) are automatically fit with the best parameters, and can be used to make predictions\n",
    "new_pred_class_rand = rand.predict(X_new)\n",
    "new_pred_class_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# also calculate predicted probabilities for the new data (for use with ensembling)\n",
    "new_pred_prob_rand = rand.predict_proba(X_new)\n",
    "new_pred_prob_rand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a submission file (score: 0.75422)\n",
    "pd.DataFrame({'id':new.id, 'cuisine':new_pred_class_rand}).set_index('id').to_csv('sub2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the mean of the predicted probabilities from KNN and RandomizedSearchCV\n",
    "new_pred_prob_combined = (new_pred_prob_knn + new_pred_prob_rand)/2\n",
    "new_pred_prob_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a list of the cuisines in alphabetical order\n",
    "cuisines = np.sort(train.cuisine.unique())\n",
    "cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert the predicted probabilities into a DataFrame\n",
    "new_pred_prob_combined = pd.DataFrame(new_pred_prob_combined, columns=cuisines)\n",
    "new_pred_prob_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each row, find the column with the highest predicted probability\n",
    "new_pred_class_combined = new_pred_prob_combined.apply(np.argmax, axis=1)\n",
    "new_pred_class_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a submission file (score: 0.75483)\n",
    "pd.DataFrame({'id':new.id, 'cuisine':new_pred_class_combined}).set_index('id').to_csv('sub3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Locating groups of similar cuisines\n",
    "\n",
    "Adapted from this [Stack Overflow question](http://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity/12128777#12128777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a document-term matrix from X using TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "X_dtm = vect.fit_transform(X)\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the cosine similarity between the first recipe and all recipes\n",
    "cosine_similarities = metrics.pairwise.linear_kernel(X_dtm[0, :], X_dtm).flatten()\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the recipe's mean similarity to each cuisine\n",
    "df = pd.DataFrame({'cuisine':train.cuisine, 'similarity':cosine_similarities})\n",
    "df[dfN['cuisine']=='greek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the recipe's mean similarity to each cuisine\n",
    "pd.DataFrame({'cuisine':train.cuisine, 'similarity':cosine_similarities}).groupby('cuisine').similarity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.loc[1, 'cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each cuisine, count the number of recipes\n",
    "cuisine_count = {}\n",
    "\n",
    "# for each cuisine, sum the mean similarity to each cuisine\n",
    "cuisine_total_similarities = {}\n",
    "\n",
    "# loop through the first 2000 recipes\n",
    "for i in range(2):\n",
    "    \n",
    "    # save the name of this recipe's cuisine\n",
    "    current_cuisine = train.loc[i, 'cuisine']\n",
    "\n",
    "    # calculate the similarity between this recipe and all other recipes\n",
    "    recipe_similarities = metrics.pairwise.linear_kernel(X_dtm[i, :], X_dtm).flatten()\n",
    "    \n",
    "    # calculate the recipe's mean similarity to each cuisine\n",
    "    cuisine_similarities = pd.DataFrame({'cuisine':train.cuisine, 'similarity':recipe_similarities}).groupby('cuisine').similarity.mean()\n",
    "    \n",
    "    # update the cuisine count and add the mean similarities\n",
    "    if current_cuisine not in cuisine_count:\n",
    "        cuisine_count[current_cuisine] = 1\n",
    "        cuisine_total_similarities[current_cuisine] = cuisine_similarities\n",
    "    else:\n",
    "        cuisine_count[current_cuisine] += 1\n",
    "        cuisine_total_similarities[current_cuisine] += cuisine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuisine_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuisine_total_similarities = pd.DataFrame(cuisine_total_similarities)\n",
    "cuisine_total_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a copy of the DataFrame\n",
    "cuisine_mean_similarities = cuisine_total_similarities.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize each column by the recipe count for each cuisine\n",
    "for col in cuisine_mean_similarities.columns:\n",
    "    cuisine_mean_similarities[col] /= cuisine_count[col]\n",
    "cuisine_mean_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display the mean similarities as a heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(cuisine_mean_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display the correlation matrix as a heatmap\n",
    "sns.heatmap(cuisine_mean_similarities.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hand-selected cuisine groups:**\n",
    "\n",
    "1. chinese, filipino, japanese, korean, thai, vietnamese\n",
    "2. british, french, irish, russian, southern_us\n",
    "3. greek, italian, moroccan, spanish\n",
    "4. brazilian, cajun_creole, indian, jamaican, mexican"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Model stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a dictionary that maps each cuisine to its group number\n",
    "cuisine_to_group = {}\n",
    "cuisine_to_group['chinese'] = 1\n",
    "cuisine_to_group['filipino'] = 1\n",
    "cuisine_to_group['japanese'] = 1\n",
    "cuisine_to_group['korean'] = 1\n",
    "cuisine_to_group['thai'] = 1\n",
    "cuisine_to_group['vietnamese'] = 1\n",
    "cuisine_to_group['british'] = 2\n",
    "cuisine_to_group['french'] = 2\n",
    "cuisine_to_group['irish'] = 2\n",
    "cuisine_to_group['russian'] = 2\n",
    "cuisine_to_group['southern_us'] = 2\n",
    "cuisine_to_group['greek'] = 3\n",
    "cuisine_to_group['italian'] = 3\n",
    "cuisine_to_group['moroccan'] = 3\n",
    "cuisine_to_group['spanish'] = 3\n",
    "cuisine_to_group['brazilian'] = 4\n",
    "cuisine_to_group['cajun_creole'] = 4\n",
    "cuisine_to_group['indian'] = 4\n",
    "cuisine_to_group['jamaican'] = 4\n",
    "cuisine_to_group['mexican'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map the cuisines to their group numbers\n",
    "train['group'] = train.cuisine.map(cuisine_to_group)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that all recipes were assigned a group\n",
    "train.group.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a \"global\" X and y, then define an X and y for each group\n",
    "X = train.ingredients_str\n",
    "y = train.group\n",
    "X1 = train.loc[train.group==1, 'ingredients_str']\n",
    "y1 = train.loc[train.group==1, 'cuisine']\n",
    "X2 = train.loc[train.group==2, 'ingredients_str']\n",
    "y2 = train.loc[train.group==2, 'cuisine']\n",
    "X3 = train.loc[train.group==3, 'ingredients_str']\n",
    "y3 = train.loc[train.group==3, 'cuisine']\n",
    "X4 = train.loc[train.group==4, 'ingredients_str']\n",
    "y4 = train.loc[train.group==4, 'cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a \"global\" pipeline, then define a pipeline for each group\n",
    "pipe_main = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "pipe_1 = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "pipe_2 = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "pipe_3 = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "pipe_4 = make_pipeline(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the cross-validated accuracy for each pipeline\n",
    "print(cross_val_score(pipe_main, X, y, cv=5, scoring='accuracy').mean())\n",
    "print(cross_val_score(pipe_1, X1, y1, cv=5, scoring='accuracy').mean())\n",
    "print(cross_val_score(pipe_2, X2, y2, cv=5, scoring='accuracy').mean())\n",
    "print(cross_val_score(pipe_3, X3, y3, cv=5, scoring='accuracy').mean())\n",
    "print(cross_val_score(pipe_4, X4, y4, cv=5, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit each pipeline with the relevant X and y\n",
    "pipe_main.fit(X, y)\n",
    "pipe_1.fit(X1, y1)\n",
    "pipe_2.fit(X2, y2)\n",
    "pipe_3.fit(X3, y3)\n",
    "pipe_4.fit(X4, y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for new data, first make group predictions\n",
    "X_new = new.ingredients_str\n",
    "new_pred_group = pipe_main.predict(X_new)\n",
    "new_pred_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# then within each predicted group, make class predictions\n",
    "new_pred_class_1 = pipe_1.predict(X_new[new_pred_group==1])\n",
    "new_pred_class_2 = pipe_2.predict(X_new[new_pred_group==2])\n",
    "new_pred_class_3 = pipe_3.predict(X_new[new_pred_group==3])\n",
    "new_pred_class_4 = pipe_4.predict(X_new[new_pred_group==4])\n",
    "print(new_pred_class_1)\n",
    "print(new_pred_class_2)\n",
    "print(new_pred_class_3)\n",
    "print(new_pred_class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add the class predictions to the DataFrame of new data\n",
    "new.loc[new_pred_group==1, 'pred_class'] = new_pred_class_1\n",
    "new.loc[new_pred_group==2, 'pred_class'] = new_pred_class_2\n",
    "new.loc[new_pred_group==3, 'pred_class'] = new_pred_class_3\n",
    "new.loc[new_pred_group==4, 'pred_class'] = new_pred_class_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a submission file (score: 0.70475)\n",
    "pd.DataFrame({'id':new.id, 'cuisine':new.pred_class}).set_index('id').to_csv('sub4.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
